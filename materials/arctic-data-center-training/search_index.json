[
["index.html", "ADC Training 1 Data Science Training for the Arctic Data Center", " ADC Training dates 1 Data Science Training for the Arctic Data Center The Arctic Data Center conducts training in data science and management, both of which are critical skills for stewardship of data, software, and other products of research that are preserved at the Arctic Data Center. 1.0.1 Acknowledgements Work on this package was supported by: NSF award #1546024 to M. B. Jones, S. Baker-Yeboah, A. Budden, J. Dozier, and M. Schildhauer Additional support was provided for working group collaboration by the National Center for Ecological Analysis and Synthesis, a Center funded by the University of California, Santa Barbara, and the State of California. "],
["data-documentation-and-publishing.html", "2 Data Documentation and Publishing 2.1 Learning Objectives 2.2 Data sharing and preservation 2.3 Data repositories: built for data (and code) 2.4 Metadata 2.5 Structure of a data package 2.6 DataONE Federation 2.7 Publishing data from the web 2.8 Publishing data from R", " 2 Data Documentation and Publishing 2.1 Learning Objectives In this lesson, you will learn: About open data archives, especially the Arctic Data Center What science metadata is and how it can be used How data and code can be documented and published in open data archives Web-based submission Submission using R 2.2 Data sharing and preservation 2.3 Data repositories: built for data (and code) GitHub is not an archival location Dedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare Rich metadata Archival in their mission Data papers, e.g., Scientific Data List of data repositories: http://re3data.org ## Arctic Data Center 2.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where it was collected, and why it was collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon data set: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And as you can see, the whole data set or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 2.5 Structure of a data package Note that the data set above lists a collection of files that are contained within the data set. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, software files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets a global identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4,and each of the individual files would have their own identfiers as well. 2.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data form the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time. 2.7 Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. Let’s walk through a web submission to see what you might expect. 2.7.1 Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://demo.arcticdata.io/#view/urn:uuid:0702cc63-4483-4af4-a218-531ccc59069f Grab both CSV files, and the R script, and store them in a convenient folder. 2.7.2 Login via ORCID We will walk through web submission on https://demo.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. When you sign in, you will be redirected to orcid.org, where you can either provide your existin ORCID credentials, or create a new account. ORCID provides multiple ways to login, including using your email address, institutional logins from many universities, and logins from social media account providers. Choose the one that is best suited to your use as a scholarly record, such as your university or agency login. 2.7.3 Create and submit the data set After signing in, you can access the data submission form using the Submit button. Once on the form, upload your data files and follow the prompts to provide the required metadata. 2.7.3.1 Click Add Files to choose the data files for your package You can select multiple files at a time to efficiently upload many files. The files will upload showing a progress indictator. You can continue editing metadata while they upload. 2.7.3.2 Enter Overview information This includes a descriptive title, abstract, and keywords. And you also must enter a funding award number and choose a license. The funding field will search for an NSF award identifier based on words in its title or the number itself. 2.7.3.3 People Information Information about the people associated with the dataset is essential to provide credit through citation and to help people understand who made contributions to the product. Enter information for the following people: Creators - all the people who should be in the citation for the dataset Contacts - one is required, but defaults to the first Creator if omitted Principal Investigators and any other that are relevant For each, please strive to provide their ORCID identifier, which helps link this dataset to their other scholarly works. 2.7.3.4 Location Information The geospatial location that the data were collected is critical for discovery and interpretation of the data. Coordinates are entered in decimal degrees, and be sure to use negative values ofor West longitudes. The editor allows you to enter multiple locations, which you should do if you had discontiguous sampling locations. This is particularly important if your sites are separated by large distances, so that spatial search will be more precise. Noite that, if you miss fields that are required, they will be highlighted in red to draw your attention. In this case, for the description, provide a comma-separated place name, ordered from the local to global: Mission Canyon, Santa Barbara, California, USA 2.7.3.5 Temporal Information Add the temporal coverage of the data, which represents the time period to which data apply. Again, use multiple date ranges if your sampling was discontinuous. 2.7.3.6 Methods Methods are critical to accurate interpretation and reuse of your data. The editor allows you to add multiple different methods sections, include details of sampling methods, experimental design, quality assurance procedures, and computational techniques and software. Please be complete with your methods sections, as they are fundamentally important to reuse of the data. 2.7.3.7 Save a first version with Submit When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. When you are successful, you should see a large green banner with a link to the current data set view. Click the X to close that banner, as we want to continue editing metadata. Success! 2.7.4 Add workflow provenance Understanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs. You add provenance by navigating to the data table descriptions, and selecting the ’Addbuttons to link the data and scripts that were used in your computational workflow. On the left side, select theAdd` circle to add an input data source to the filteredSpecies.csv file. This starts building the provenance graph to explain the origin and history of each data object. The linkage to the source data set should appear. Then you can add the link to the source code that handled the conversion between the data files by clicking on Add arrow and selecting the R script: The diagram now shows the relationships among the data files and the R script, so click Submit to save another version of the package, and clear the success dialog to continue editing metadata. 2.7.5 File and variable level metadata The final major section of metadata concerns the structure and contents of your data files. In this case, provide the names and descriptions of the data contained in each file, as well as details of their internal structure. For example, for data tables, you’ll need the name, label, and definition of each variable in your file. Click the Describe button to access a dialog to enter this information. The Attributes tab is where you enter variable (aka attribute) information, including: variable name (for programs) variable label (for display) - variable definition (be specific) - type of measurement - units - code definitions for values - missing value codes You’ll need to add these definitions for every variable (column) in the file. When done, click Done. Now the list of data files will show a green checkbox indicating that you have full described that file’s internal structure. Proceed with the other CSV files, and then click Submit Dataset to save all of these changes. After you get the big green success message, you can visit your dataset and review all of the information that you provided. If you find any errors, simply click Edit again to make changes. Et voilà! A beatifully preserved data package! 2.8 Publishing data from R Now lets see how to use the dataone and datapack R packages to upload data to DataONE member repositories like the KNB Data Repository and the Arctic Data Center. The dataone R package provides methods to enable R scripts to interact with DataONE to search for, download, upload and update data and metadata. The purpose of uploading data from R is to automate the repetitive tasks for large data sets with many files. For small data sets, the web submission for will certainly be simpler. The dataone R package represents the set of files in a data set as a datapack::DataPackage. We will create a DataPackage locally, and then upload it to a test version of the Arctic Data Center repository using dataone. 2.8.1 Logging in Before uploading any data to a DataONE repository, you must login to get an authentication token, which is a character string used to identify yourself. This token can be retrieved by logging into the test repository and copying the token into your R session. 2.8.2 Obtain an ORCID ORCID is a researcher identifier that provides a common way to link your researcher identity to your articles and data. An ORCID is to a researcher as a DOI is to a research article. To obtain an ORCID, register at https://orcid.org. 2.8.3 Log in to the test repository and copy your token We will be using a test server, so login and retrieve your token at https://demo.arcticdata.io Once you are logged in, navigate to your Profile Settings, and locate the “Authentication Token” section, and then copy the token for R to your clipboard. Finally, paste the token into the R Console to register it as an option for this R session. You are now logged in. But note that you need to keep this token private; don’t paste it into scripts or check it into Git, as it is just as sensitive as your password. 2.8.4 Modifying metadata Next, modify the metadata file associated with the package to set yourself as the owner. This will help us differentiate the test data later. Open the strix-pacific-northwest.xml file in RStudio, and change the givenName and surName fields at the top to your name. library(EML) source(&quot;misc/eml_helpers.R&quot;) # Load the EML file into R emlFile &lt;- &quot;data/strix/strix-pacific-northwest.xml&quot; doc &lt;- read_eml(emlFile) # Change creator to us doc@dataset@creator &lt;- c(eml_creator(&quot;Matthew&quot;, &quot;Jones&quot;, email = &quot;jones@nceas.ucsb.edu&quot;)) # Change abstract to the better one we wrote doc@dataset@abstract &lt;- as(set_TextType(&quot;data/strix/better-abstract.md&quot;), &quot;abstract&quot;) # Save it back to the filesystem write_eml(doc, emlFile) 2.8.5 Uploading A Package Using R with uploadDataPackage Datasets and metadata can be uploaded individually or as a collection. Such a collection, whether contained in local R objects or existing on a DataONE repository, will be informally referred to as a package. The steps necessary to prepare and upload a package to DataONE using the uploadDataPackage method will be shown. A complete script that uses these steps is shown here. In the first section, we create a ’DataPackage as a container for our data and metadata and scripts. It starts out as empty. library(dataone) library(datapack) library(uuid) d1c &lt;- D1Client(&quot;STAGING2&quot;, &quot;urn:node:mnTestKNB&quot;) dp &lt;- new(&quot;DataPackage&quot;) show(dp) We then add a metadata file, data file, R script and output data file to this package. Our first order is to generate identifiers for the files that are part of the package, and add EML metadata that references those identifiers. # Generate identifiers for our data and program objects, and add them to the metadata sourceId &lt;- paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) progId &lt;- paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) outputId &lt;- paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) doc@dataset@otherEntity[[1]]@id &lt;- new(&quot;xml_attribute&quot;, sourceId) doc@dataset@otherEntity[[2]]@id &lt;- new(&quot;xml_attribute&quot;, progId) doc@dataset@otherEntity[[3]]@id &lt;- new(&quot;xml_attribute&quot;, outputId) repo_obj_service &lt;- paste0(d1c@mn@endpoint, &quot;/object/&quot;) doc@dataset@otherEntity[[1]]@physical[[1]]@distribution[[1]]@online@url &lt;- new(&quot;url&quot;, paste0(repo_obj_service, sourceId)) doc@dataset@otherEntity[[2]]@physical[[1]]@distribution[[1]]@online@url &lt;- new(&quot;url&quot;, paste0(repo_obj_service, progId)) doc@dataset@otherEntity[[3]]@physical[[1]]@distribution[[1]]@online@url &lt;- new(&quot;url&quot;, paste0(repo_obj_service, outputId)) write_eml(doc, emlFile) Now we have a full metadata document ready be uploaded. In the next section, we’ll add the data files and metadata to a DataPackage, and then upload that to a test repository. # Add the metadata document to the package metadataObj &lt;- new(&quot;DataObject&quot;, format=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;, filename=paste(getwd(), emlFile, sep=&quot;/&quot;)) dp &lt;- addMember(dp, metadataObj) # Add our input data file to the package sourceData &lt;- &quot;data/strix/sample.csv&quot; sourceObj &lt;- new(&quot;DataObject&quot;, id = sourceId, format=&quot;text/csv&quot;, filename=paste(getwd(), sourceData, sep=&quot;/&quot;)) dp &lt;- addMember(dp, sourceObj, metadataObj) # Add our processing script to the package progFile &lt;- &quot;data/strix/filterSpecies.R&quot; progObj &lt;- new(&quot;DataObject&quot;, id = progId, format=&quot;application/R&quot;, filename=paste(getwd(), progFile, sep=&quot;/&quot;), mediaType=&quot;text/x-rsrc&quot;) dp &lt;- addMember(dp, progObj, metadataObj) # Add our derived output data file to the package outputData &lt;- &quot;data/strix/filteredSpecies.csv&quot; outputObj &lt;- new(&quot;DataObject&quot;, id = outputId, format=&quot;text/csv&quot;, filename=paste(getwd(), outputData, sep=&quot;/&quot;)) dp &lt;- addMember(dp, outputObj, metadataObj) myAccessRules &lt;- data.frame(subject=&quot;http://orcid.org/0000-0003-0077-4738&quot;, permission=&quot;changePermission&quot;) # Add the provenance relationships to the data package dp &lt;- describeWorkflow(dp, sources=sourceObj, program=progObj, derivations=outputObj) show(dp) Finally, we upload the package to the Testing server for the KNB. packageId &lt;- uploadDataPackage(d1c, dp, public=TRUE, accessRules=myAccessRules, quiet=FALSE) This particular package contains the R script filterSpecies.R, the input file sample.csv that was read by the script and the output file filteredSpecies.csv that was created by the R script, which was run at a previous time. You can now search for and view the package at https://dev.nceas.ucsb.edu: In addition, each of the uploaded entities shows the relevant provenance information, showing how the source data is linked to the derived data via the R program that was used to process the raw data: "],
["data-packaging.html", "3 Data Packaging 3.1 Upload a single data file 3.2 Upload a simple Data Package", " 3 Data Packaging This document briefly covers two common tasks: Uploading a data file to the Arctic Data Center Uploading an entire Data Package to the Arctic Data Center The R dataone package offers some convenient ways to do this, particularly some custom R classes which encapsulate a lot of the complexity of Objects and Data Packages. First, let’s load our usual libraries and connect to our Member Node: library(dataone) library(datapack) Here we use a D1Client which is another way of connecting to a Member Node: client &lt;- D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) Alternatively, we could also create an MNode object directly. 3.1 Upload a single data file First let’s make an example data file (CSV in this case) to upload: data(&quot;co2&quot;) # Built-in dataset in R write.csv(co2, &quot;./co2.csv&quot;) Every Object we upload then just needs a DataObject class instance for it: my_object &lt;- new(&quot;DataObject&quot;, filename = &quot;./co2.csv&quot;, format = &quot;text/csv&quot;) my_object &lt;- setPublicAccess(my_object) # Make it public readable! And then we just need to upload it: uploadDataObject(client, my_object) 3.2 Upload a simple Data Package The steps to upload an entire package aren’t that much more complex. First, let’s create an example EML file: library(EML) title &lt;- &quot;Test dataset to show submitting via R client&quot; me &lt;- as.person(&quot;Bryce Mecum &lt;mecum@nceas.ucsb.edu&gt;&quot;) dataset &lt;- new(&quot;dataset&quot;, title = title, creator = me, contact = me) eml_pid &lt;- paste0(&quot;urn:uuid&quot;, uuid::UUIDgenerate()) eml &lt;- new(&quot;eml&quot;, packageId = eml_pid, system = &quot;uuid&quot;, dataset = dataset) eml_path &lt;- &quot;~/my_eml.xml&quot; write_eml(eml, eml_path) eml_validate(eml_path) And then we just use the DataPackage class: my_package &lt;- new(&quot;DataPackage&quot;) my_metadata &lt;- new(&quot;DataObject&quot;, format = &quot;eml://ecoinformatics.org/eml-2.1.1&quot;, filename = eml_path) my_object &lt;- setPublicAccess(my_object) # Make it public readable! my_object &lt;- new(&quot;DataObject&quot;, filename = &quot;./co2.csv&quot;, format = &quot;text/csv&quot;) my_object &lt;- setPublicAccess(my_object) # Make it public readable! addData(my_package, my_metadata) addData(my_package, my_object, mo = my_metadata) uploadDataPackage(client, my_package, public = TRUE) "],
["query-and-download.html", "4 Query and Download 4.1 Query with R 4.2 Find data on thawing and download it all", " 4 Query and Download The Arctic Data Center supports the DataONE API which means it lets us have programattic access to most aspects of the system: Find data Get data Submit data Finding data through the DataONE API differs from searching directly through our website in that the DataONE API supports a much richer set of query options than our website. Like many DataONE member nodes, the Arctic Data Center runs a Solr index for easy querying which can be queried so long as you know how to use Solr and where to point your query. You can use your web browser, curl, any Solr library you prefer or the dataone R package to query the Arctic Data Center. To get a list of fields you can query, visit: https://arcticdata.io/metacat/d1/mn/v2/query/solr (Either in your web browser or your programming language of choice) And to query the Solr endpoint, you can do something like this: https://arcticdata.io/metacat/d1/mn/v2/query/solr/?q=. which returns any Object stored in the index. 4.1 Query with R Let’s see how the dataone package can be used to query and download data objects from the Arctic Data Center. First, load the dataone package: library(dataone) Then we need to specify the Member Node we want to query (Arctic Data Center in this case): cn &lt;- CNode(&quot;PROD&quot;) mn &lt;- getMNode(cn, &quot;urn:node:ARCTIC&quot;) Every Solr query is made up of a set of parameters which we need to set up. An R list is one of the easiest ways to do this: params &lt;- list( &quot;q&quot; = &quot;*:*&quot;, &quot;rows&quot; = &quot;5&quot;, &quot;fl&quot; = &quot;identifier,formatId&quot; ) And then the query is run with the conventiently-named query function: query(mn, params, as = &quot;data.frame&quot;) By default, query returns a list but you can see in the above output and code that I specified that query should return a data.frame instead. 4.2 Find data on thawing and download it all Instead of simply querying the Arctic Data Center for data, perhaps we want to download the data we found. The dataone package supports this easily. Let’s say we want to download data related to thaw depth. First, we generate and run a query for the five most recent datasets with ‘thaw’ in their title: params &lt;- list( &quot;q&quot; = &quot;title:*thaw*+AND+formatType:METADATA+-obsoletedBy:*&quot;, &quot;rows&quot; = &quot;5&quot;, &quot;fl&quot; = &quot;identifier,title,resourceMap&quot;, &quot;sort&quot; = &quot;dateUploaded+desc&quot; ) results &lt;- query(mn, params) results How could we go about downloading the data in one of these datasets? Let’s start with just one. The basic idea is that each dataset is contained within a resource map, which is the container for the metadata and its related data. When a data object is part of a resource map, it will have a resourceMap field set for it in the Solr index. We can query this like so: resource_map_pid &lt;- results[[1]]$resourceMap[[1]] params &lt;- list( &quot;q&quot; = paste0(&#39;resourceMap:&quot;&#39;, resource_map_pid, &#39;&quot;+AND+formatType:DATA+-obsoletedBy:*&#39;), &quot;rows&quot; = &quot;1000&quot;, &quot;fl&quot; = &quot;identifier,formatId,fileName,dataUrl&quot;) just_data &lt;- query(mn, params, as = &quot;data.frame&quot;) just_data Now that we know the PID of the data objects in this particular dataset, we just need one more line of code to actually download it: writeBin(getObject(mn, just_data[1,&quot;identifier&quot;]), just_data[1,&quot;fileName&quot;]) If we want to do this in bulk, we only need to use for loops or apply function calls to do what we did above but for each dataset, and for each data file in each dataset: lapply(results, function(dataset) { cat(paste0(&quot;Downloading data for &quot;, dataset$title, &quot;\\n&quot;)) params &lt;- list(&quot;q&quot; = paste0(&#39;resourceMap:&quot;&#39;, dataset$resourceMap[[1]], &#39;&quot;+AND+formatType:DATA+-obsoletedBy:*&#39;), &quot;rows&quot; = &quot;1&quot;, &quot;fl&quot; = &quot;identifier,formatId,fileName,dataUrl&quot;) just_data &lt;- query(mn, params, as = &quot;data.frame&quot;) if (nrow(just_data) == 0) { return(list()) } paths &lt;- lapply(seq_len(nrow(just_data)), function(i) { cat(paste0(&quot; Downloading data file &quot;, just_data[i,&quot;identifier&quot;], &quot;\\n&quot;)) data_path &lt;- tempfile() writeBin(getObject(mn, just_data[i,&quot;identifier&quot;]), data_path) data_path }) cat(&quot;\\n&quot;) paths }) "]
]
